# -*- coding: utf-8 -*-
"""
Created on Sat Mar 12 03:42:00 2022

@author: UTKARSH
"""

import json
import uuid
import glob
from collections import defaultdict


TEXT_MATCH_LENGTH = 100
TAGGERS_THRESHOLD = 3
REVIEWERS_THRESHOLD = 2


def extract_relevant_fields(doc):
    """
    Extracts just the relevant fields from the json generated by label-studio
    and discards all the irrelevant fields for calculating scores

    Parameters
    ----------
    doc : dict
        The dictionary element in label-studio output format.

    Returns
    -------
    dict
        Cleaned dictionary with just the relevant fields.

    """
    entities = []
    for entity in doc["annotations"][0]["result"]:
        try:
            entities.append({
                "start": entity["value"]["start"],
                "end": entity["value"]["end"],
                "span": (entity["value"]["start"], entity["value"]["end"]),
                "text": entity["value"]["text"],
                "label": entity["value"]["labels"][0]
            })
        except:
            print(entity["value"])

    return {
        "entities": entities,
        "text": doc["data"]["text"]
    }

    

def match_by_doc_text(text, docs):
    """
    Aligns a document with it's match from a list of documents by matching text
    string

    Parameters
    ----------
    text : str
        Text from a judgement document.
    docs : list
        A list of documents to match from.

    Returns
    -------
    doc : dict
        Matching document with all relevant fields and document text.

    """
    for doc in docs:
        if doc["text"][:TEXT_MATCH_LENGTH] == text[:TEXT_MATCH_LENGTH]:
            return doc
    assert 1 == 2  # Just to make sure the match is always found in the list


def get_aligned_docs_list(docs_dict, annotator):
    """
    Aligns all the documents annotated by annotator and reviewed by reviewers
    to get agreement scores

    Parameters
    ----------
    docs_dict : dict
        A dictionary where keys are the names of taggers (annotator/reviewers)
        and values are a list of documents.
    annotator : str
        Name of the annotator who annotated the document.

    Returns
    -------
    aligned_docs_list : list
        A list of documents with document text and all the tagged entities by
        annotator and reviewers.

    """
    assert len(docs_dict) > 1
    print(annotator)
    for _, docs in docs_dict.items():
        for doc_i, doc in enumerate(docs):
            docs[doc_i] = extract_relevant_fields(doc)

    aligned_docs_list = list()

    for doc_ref in docs_dict[annotator]:
        reviewed_entities = dict()

        for reviewer, docs in docs_dict.items():
            if reviewer == annotator:
                continue
            matching_doc = match_by_doc_text(doc_ref["text"], docs)
            reviewed_entities[reviewer] = matching_doc["entities"]

        assert len(reviewed_entities) == len(docs_dict) - 1

        aligned_docs_list.append({
            "doc_id": str(uuid.uuid1()),
            "text": doc_ref["text"],
            "ref_entities": doc_ref["entities"],
            "reviewed_entities": reviewed_entities
        })

    return aligned_docs_list


def get_span_label_list(entities):
    """
    Get a list of tuples of all the spans and labels from a list of entities

    Parameters
    ----------
    entities : list
        A list of all the entities by tagger

    Returns
    -------
    list
        A list of tuples of spans and labels for all the entities.

    """
    return [(entity["span"], entity["label"]) for entity in entities]


def calculate_inter_annotator_score(aligned_docs_list, annotator):
    """
    Calculates total annotated, total correctly annotated and total missed
    number of documents by a particular annotator

    Parameters
    ----------
    aligned_docs_list : list
        A list of all the document with document text and aligned entities.
    annotator : str
        Name of the annotator.

    Returns
    -------
    total_annotated : int
        Total number of annotated entities by the annotator.
    total_correctly_annotated : int
        Total number of correctly annotated entities by the annotator.
    total_missed : int
        Total number of missed entities by the annotator.

    """
    total_annotated = 0
    total_correctly_annotated = 0
    total_missed = 0
    
    for aligned_doc in aligned_docs_list:
        
        spans_dict = defaultdict(set)

        for span in get_span_label_list(aligned_doc["ref_entities"]):
            spans_dict[span].add(annotator)

        total_annotated += len(spans_dict)

        for reviewer, reviewed_entities in aligned_doc["reviewed_entities"].items():
            for span in get_span_label_list(reviewed_entities):
                spans_dict[span].add(reviewer)
        for reviewers in spans_dict.values():
            if annotator in reviewers and len(reviewers) >= TAGGERS_THRESHOLD:
                total_correctly_annotated += 1
            if annotator not in reviewers and len(reviewers) >= REVIEWERS_THRESHOLD:
                total_missed += 1

    return total_annotated, total_correctly_annotated, total_missed


def get_f1_score(total_annotated, total_correctly_annotated, total_missed):
    """
    Calculates inter-annotator precision, recall and f1-scores for all documents
    annotated by the annotator and reviewed by the reviewers

    Parameters
    ----------
    total_annotated : int
        Total number of annotated entities by the annotator.
    total_correctly_annotated : int
        Total number of correctly annotated entities by the annotator.
    total_missed : int
        Total number of missed entities by the annotator.

    Returns
    -------
    precision : float
        Precision agreement score.
    recall : float
        Recall agreement score.
    f1 : float
        f1 agreement score.

    """
    precision = total_correctly_annotated / total_annotated
    recall = total_correctly_annotated / (total_correctly_annotated + total_missed)
    f1 = 2 * precision * recall / (precision + recall)
    return precision, recall, f1


if __name__ == "__main__":

    paths = list()

    for folder in glob.glob("data/annotations/*"):
        paths.extend(glob.glob(folder + "/*"))

    total_annotated_entities, total_correctly_annotated_entities, total_missed_entities = 0, 0, 0

    for oks_path, sne_path, utk_path in zip(paths[1:5], paths[5:9], paths[9:13]):
        docs_dict = dict()
        taggers = list()

        for ann_path in [oks_path, sne_path, utk_path]:

            with open(ann_path, encoding="utf-8") as f:
                docs = json.load(f)
            
            tagger_title = ann_path.split("\\")[-1].replace(".json", "")
            taggers.append(tagger_title)

            docs_dict[tagger_title] = docs
            
            annotator = min(taggers, key=len)
        
        aligned_docs_list = get_aligned_docs_list(docs_dict, annotator=annotator)
        total_annotated, total_correctly_annotated, total_missed = (
            calculate_inter_annotator_score(aligned_docs_list, annotator=annotator)
        )
        
        total_annotated_entities += total_annotated
        total_correctly_annotated_entities += total_correctly_annotated
        total_missed_entities += total_missed

    precision, recall, f1 = get_f1_score(
        total_annotated_entities, total_correctly_annotated_entities, total_missed_entities
    )
    
    print(f"Total number of annotated entities: {total_annotated_entities}")
    print(f"Total number of correctly annotated entities: {total_correctly_annotated_entities}")
    print(f"Total number of missed entities: {total_missed_entities}")

    print(f"Our final agreement precision: {round(precision, 3)}")
    print(f"Our final agreement recall: {round(recall, 3)}")
    print(f"Our final agreement f1-score: {round(f1, 3)}")
        