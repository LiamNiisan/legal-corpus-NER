{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "\n",
    "sys.path.append(os.path.join(os.path.pardir, \"src\"))\n",
    "from interannotator_agreement import get_aligned_docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(\"__file__\")),\n",
    "    os.path.pardir,\n",
    "    \"src\",\n",
    "    \"data\",\n",
    "    \"annotations\",\n",
    ")\n",
    "\n",
    "oksana_docs = {\n",
    "    \"oksana\": os.path.join(annotation_path, \"oksana\", \"oksana.json\"),\n",
    "    \"sneha\": os.path.join(annotation_path, \"sneha\", \"oksana_sneha.json\"),\n",
    "    \"utkarsh\": os.path.join(annotation_path, \"utkarsh\", \"oksana_utkarsh.json\"),\n",
    "    \"badr\": os.path.join(annotation_path, \"badr\", \"oksana_badr.json\")\n",
    "}\n",
    "\n",
    "sneha_docs = {\n",
    "    \"sneha\": os.path.join(annotation_path, \"sneha\", \"sneha.json\"),\n",
    "    \"oksana\": os.path.join(annotation_path, \"oksana\", \"sneha_oksana.json\"),\n",
    "    \"utkarsh\": os.path.join(annotation_path, \"utkarsh\", \"sneha_utkarsh.json\"),\n",
    "    \"badr\": os.path.join(annotation_path, \"badr\", \"sneha_badr.json\")\n",
    "}\n",
    "\n",
    "utkarsh_docs = {\n",
    "    \"utkarsh\": os.path.join(annotation_path, \"utkarsh\", \"utkarsh.json\"),\n",
    "    \"oksana\": os.path.join(annotation_path, \"oksana\", \"utkarsh_oksana.json\"),\n",
    "    \"sneha\": os.path.join(annotation_path, \"sneha\", \"utkarsh_sneha.json\"),\n",
    "    \"badr\": os.path.join(annotation_path, \"badr\", \"utkarsh_badr.json\")\n",
    "}\n",
    "\n",
    "badr_docs = {\n",
    "    \"badr\": os.path.join(annotation_path, \"badr\", \"badr.json\"),\n",
    "    \"oksana\": os.path.join(annotation_path, \"oksana\", \"badr_oksana.json\"),\n",
    "    \"sneha\": os.path.join(annotation_path, \"sneha\", \"badr_sneha.json\"),\n",
    "    \"utkarsh\": os.path.join(annotation_path, \"utkarsh\", \"badr_utkarsh.json\")\n",
    "}\n",
    "\n",
    "annotators = {\n",
    "    \"oksana\": oksana_docs,\n",
    "    \"sneha\": sneha_docs,\n",
    "    \"utkarsh\": utkarsh_docs,\n",
    "    \"badr\": badr_docs,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_triples(doc, name=\"name\"):\n",
    "    triples = []\n",
    "    for anot in doc:\n",
    "        span = anot[\"span\"]\n",
    "        triples.append((name, span, anot[\"label\"]))\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_docs_dict = {}\n",
    "for annotator, docs in annotators.items():\n",
    "    annotations = dict()\n",
    "    for annotator2, path in docs.items():\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            annotations[annotator2] = json.load(f)\n",
    "    aligned_docs = get_aligned_docs_list(annotations, annotator)\n",
    "    aligned_docs_dict[annotator] = aligned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_score = []\n",
    "pi_score = []\n",
    "K_score = []\n",
    "alpha_score = []\n",
    "for annotator, docs in aligned_docs_dict.items():\n",
    "    for doc in docs:\n",
    "        triples = doc_to_triples(doc[\"ref_entities\"], annotator)\n",
    "        for reviewer, review_doc in doc[\"reviewed_entities\"].items():\n",
    "            triples += doc_to_triples(review_doc, reviewer)\n",
    "\n",
    "        spans = set([span for name, span, tag in triples])\n",
    "        names = set([name for name, span, tag in triples])\n",
    "\n",
    "        for anot_name in names:\n",
    "            spans_name = set([span for name, span, tag in triples if name == anot_name])\n",
    "            if len(spans_name) != len(spans):\n",
    "                not_cross = spans - (spans_name & spans)\n",
    "                for span in not_cross:\n",
    "                    triples.append((anot_name, span, \"UNTAGGED\"))\n",
    "\n",
    "        task = AnnotationTask(triples)        \n",
    "\n",
    "        S_score.append(task.S())\n",
    "        pi_score.append(task.pi())\n",
    "        K_score.append(task.kappa())\n",
    "        alpha_score.append(task.alpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S score : 0.9470038574679591\n",
      "pi score : 0.9366619900370948\n",
      "K score : 0.9371439741077994\n",
      "Alpha score : 0.9360954383660536\n"
     ]
    }
   ],
   "source": [
    "print(f\"S score : {np.mean(S_score)}\")\n",
    "print(f\"pi score : {np.mean(pi_score)}\")\n",
    "print(f\"K score : {np.mean(K_score)}\")\n",
    "print(f\"Alpha score : {np.mean(alpha_score)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8220b76cfe9388742b41023fa80cc7118ae1a5c202f89a323335059602ac9bc"
  },
  "kernelspec": {
   "display_name": "Python [conda env:env_text]",
   "language": "python",
   "name": "conda-env-env_text-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
