# Interannotator Agreement Study

## Agreement score

We chose Scott's $\pi$ score for our annotation.
The reason for that is that we consider that all our tags are equal, but at the same time, the tags are not equally distributed, but we assume that since we have a big sample size of randomly sampled similar documents shared between annotators, the distribution of the tags will be similar for each annotator.

### Results

We got a $\pi$ score of: `0.92`, which is pretty high!
The tags follows patterns which are unambiguous most of the time and when they aren't, we discuss them.
Sometimes, there are some ambiguities and those deffinitely are a reason for a lowered score. 
They other reason is human error, we had lots of documents to tag and some of us deffinitively didn't make the perfect choice each time.